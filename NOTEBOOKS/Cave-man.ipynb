{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6018f70",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ecf3008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69faa846",
   "metadata": {},
   "source": [
    "## About Environment\n",
    "* `*` represents obstructions\n",
    "* `M` represents the Man that is trying to eat the food and avoid the obstruction\n",
    "* `F` represents Food placed randomly\n",
    "\n",
    ">> `Man` `Food` and obstruction other than wall placed randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "df27761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class env:\n",
    "    \n",
    "    def __init__(self, N, wall_points = 3):\n",
    "        self.square = np.zeros(shape = (N, N), dtype = np.int8)\n",
    "        self.N = N\n",
    "        self.ag = [np.random.randint(1, N - 1) , np.random.randint(0, N - 1)]\n",
    "        self.square[tuple(self.ag)] = 2\n",
    "        # select a point randomly (in middle somewhat)\n",
    "        walls = []\n",
    "        for _ in range(wall_points):\n",
    "            i, j = np.random.randint(0, N) , np.random.randint(0, N)\n",
    "            self.square[i][j] = 1\n",
    "        # mark wall on side\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                if i== 0 or j == 0 or i == N -1 or j == N - 1:\n",
    "                    self.square[i][j] = 1\n",
    "          \n",
    "        # get the agent place\n",
    "        \n",
    "        self.food = [np.random.randint(1, N-1),  np.random.randint(1, N-1)]\n",
    "        self.square[tuple(self.food)] = 3\n",
    "                    \n",
    "    def showField(self):\n",
    "        for i in range(self.N):\n",
    "            for j in range(self.N):\n",
    "                if self.square[i][j] == 1:\n",
    "                    print('*', end = ' ')\n",
    "                elif tuple(self.ag) == (i , j):\n",
    "                    print('M', end = ' ')\n",
    "                elif tuple(self.food) == (i , j):\n",
    "                    print(\"F\", end = ' ')\n",
    "                else:\n",
    "                    print(' ', end = ' ')\n",
    "            print()\n",
    "\n",
    "    def getFeature(self):\n",
    "        '''\n",
    "        features = [threat_up, threat_right, threat_down, threat_left, Is_food_up, Is_food_right, Is_food_down, Is_food_left]  # features, done, reward\n",
    "        '''\n",
    "        if self.square[tuple(self.ag)] == 3: # eaten the food\n",
    "            return [self.square[self.ag[0] - 1, self.ag[1]] == 1, self.square[self.ag[0], self.ag[1] + 1] == 1,\n",
    "                             self.square[self.ag[0] + 1, self.ag[1]] == 1, self.square[self.ag[0], self.ag[1] - 1] == 1,\n",
    "                             self.ag[0] > self.food[0], self.food[1] > self.ag[1], self.ag[0] < self.food[0], \n",
    "                             self.food[1] <self.ag[1]], 1, 10\n",
    "        elif self.square[tuple(self.ag)] == 1:\n",
    "            return [True, True, True, True, False, False, False, False],  1, -1\n",
    "        else:\n",
    "            return  [self.square[self.ag[0] - 1, self.ag[1]] == 1, self.square[self.ag[0], self.ag[1] + 1] == 1,\n",
    "                             self.square[self.ag[0] + 1, self.ag[1]] == 1, self.square[self.ag[0], self.ag[1] - 1] == 1,\n",
    "                             self.ag[0] > self.food[0], self.food[1] > self.ag[1], self.ag[0] < self.food[0], \n",
    "                             self.food[1] <self.ag[1]], 0, 0        \n",
    "        \n",
    "    def step(self, action):\n",
    "        '''\n",
    "        Do the action\n",
    "        '''\n",
    "        if action == 0: # up mve\n",
    "            self.ag[0] -= 1\n",
    "        elif action == 1:\n",
    "            self.ag[1] += 1\n",
    "        elif action == 2:\n",
    "            self.ag[0] += 1\n",
    "        elif action == 3:\n",
    "            self.ag[1] -= 1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96d390dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* * * * * * * * * * * * * * * \n",
      "*                           * \n",
      "*                           * \n",
      "*                           * \n",
      "*                           * \n",
      "*                           * \n",
      "*                           * \n",
      "*                   M       * \n",
      "*                           * \n",
      "*                       *   * \n",
      "*                           * \n",
      "*                           * \n",
      "*                           * \n",
      "*     F             *       * \n",
      "* * * * * * * * * * * * * * * \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([False, False, False, False, False, False, True, True], 0, 0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyEnv = env(15)\n",
    "MyEnv.showField()\n",
    "MyEnv.getFeature()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ff11da",
   "metadata": {},
   "source": [
    "## Features :\n",
    "`features = [threat_up, threat_right, threat_down, threat_left, Is_food_up, Is_food_right,Is_food_down, Is_food_left ]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78ea314d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* * * * * * * * * * * * * * * \n",
      "*                           * \n",
      "*         M                 * \n",
      "*                           * \n",
      "*                           * \n",
      "*                           * \n",
      "*                           * \n",
      "*     F                     * \n",
      "*                           * \n",
      "*                           * \n",
      "*       *   *         *     * \n",
      "*                           * \n",
      "*                           * \n",
      "*                           * \n",
      "* * * * * * * * * * * * * * * \n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "# Your loop\n",
    "for i in range(5):\n",
    "    # Clear the previous output\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Your new content to be displayed\n",
    "    MyEnv = env(15)\n",
    "    MyEnv.showField()\n",
    "    \n",
    "    # Pause for a short duration (optional)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Display the updated content\n",
    "    display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27349a8b",
   "metadata": {},
   "source": [
    "## Buffer space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d3008c",
   "metadata": {},
   "source": [
    ">> We are using replay buffer of size 10K that contains experience <S, A, R, S', DONES>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c18bc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "replay_buffer = deque(maxlen = 100_00) # Buffer space\n",
    "NUM_INPUTS = 8\n",
    "HIDDEN_UNITS = 20\n",
    "OUTPUTS = 4\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6acc5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CaveNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.inputLayer = nn.Linear(in_features=NUM_INPUTS, out_features=HIDDEN_UNITS)\n",
    "        self.hiddenLayer = nn.Linear(in_features=HIDDEN_UNITS, out_features=HIDDEN_UNITS)\n",
    "        self.outputLayer = nn.Linear(in_features=HIDDEN_UNITS, out_features= OUTPUTS)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.inputLayer(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.hiddenLayer(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.hiddenLayer(x)\n",
    "        x = self.relu(x)\n",
    "        out = self.outputLayer(x)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4507786b",
   "metadata": {},
   "source": [
    "### Copy model\n",
    ">> Target model provides stability while training and will be updated after certain period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac847c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "model = CaveNet()\n",
    "target_model = CaveNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "23bf425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def epsilon_greedy_action_selection(model, epsilon, observation, sharpening_factor = 1):\n",
    "    if np.random.random() > epsilon:\n",
    "        prediction = model(torch.tensor(observation, dtype = torch.float))  # perform the prediction on the observation\n",
    "        # Chose the action from softmax distribution\n",
    "        action = torch.multinomial(F.softmax(prediction*sharpening_factor, dim = 0), num_samples = 1).item()   \n",
    "    else:\n",
    "        action = np.random.randint(0, 4)  # Else use random action\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42779a65",
   "metadata": {},
   "source": [
    "### Training The mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "069963ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize = torch.optim.SGD(params = model.parameters(), lr = 0.1)\n",
    "loss_fn = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0b87d599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(states, target, model = model, EPOCHS = 1, batch_size = BATCH_SIZE):\n",
    "    for epoch in range(EPOCHS):\n",
    "        # train mode on\n",
    "        model.train()\n",
    "        # forward prop\n",
    "        y_preds = model(states)\n",
    "        loss = loss_fn(y_preds, target)\n",
    "        #optimizer zero grading\n",
    "        optimizer.zero_grad()\n",
    "        # backprop the loss\n",
    "        loss.backward()\n",
    "        #optimizer step\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9083b45",
   "metadata": {},
   "source": [
    "## Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6d96e5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay(replay_buffer,batch_size = BATCH_SIZE, model = model, target_model = target_model ):\n",
    "    \n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return\n",
    "    samples = random.sample(replay_buffer, batch_size)\n",
    "    target_batch = []  \n",
    "    \n",
    "    zipped_samples = list(zip(*samples))  \n",
    "    states, actions, rewards, new_states, dones = zipped_samples\n",
    "    states, new_states = torch.tensor(np.array(states), dtype = torch.float), torch.tensor(np.array(new_states), dtype = torch.float)\n",
    "    with torch.inference_mode():\n",
    "        # Predict targets for all states from the sample\n",
    "        targets = target_model(states)\n",
    "        # Predict Q-Values for all new states from the sample\n",
    "        q_values = model(new_states)\n",
    "    for i in range(batch_size):\n",
    "        # take the maximum value\n",
    "        q_val = max(q_values[i])\n",
    "        target = torch.clone(targets[i]).numpy()\n",
    "        if dones[i]:\n",
    "            target[actions[i]] = rewards[i]\n",
    "        else:\n",
    "            target[actions[i]] = rewards[i] + q_val * GAMMA\n",
    "        \n",
    "        target_batch.append(target)\n",
    "    train(states, torch.tensor(target_batch))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "445733e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(params = model.parameters(), lr = 0.1)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "EPSILON = 1.0\n",
    "EPSILON_REDUCE = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0d973b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model = model, target_model = target_model,EPSILON = EPSILON, EPSILON_REDUCE = EPSILON_REDUCE,EPOCHS = 1):\n",
    "\n",
    "    \n",
    "    num_done = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        MyEnv = env(15) # initialsation of environment\n",
    "        state, done, reward = MyEnv.getFeature()\n",
    "        num_simulation = 0 # we will simulate an episode for maximum of 100 steps\n",
    "        while not done:\n",
    "            # choose an action\n",
    "            action = epsilon_greedy_action_selection(model, EPSILON, state)\n",
    "            # perform action and get next state\n",
    "            MyEnv.step(action)\n",
    "            new_state, done, reward = MyEnv.getFeature()\n",
    "            replay_buffer.append((state, action, reward, new_state, done))\n",
    "            state = new_state\n",
    "            if reward == 10:\n",
    "                num_done += 1\n",
    "            num_simulation += 1\n",
    "            if num_simulation >= 100:\n",
    "                break\n",
    "            \n",
    "        replay(replay_buffer)  \n",
    "       \n",
    "        EPSILON *= EPSILON_REDUCE\n",
    "        \n",
    "        if epoch % 500 == 0:\n",
    "            target_model.load_state_dict(model.state_dict())\n",
    "            print(f\" {epoch} : DONES = {num_done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3597499c",
   "metadata": {},
   "source": [
    "* Training the model for 100k epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "20df0b7f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 : DONES = 1\n",
      " 500 : DONES = 258\n",
      " 1000 : DONES = 583\n",
      " 1500 : DONES = 936\n",
      " 2000 : DONES = 1318\n",
      " 2500 : DONES = 1716\n",
      " 3000 : DONES = 2132\n",
      " 3500 : DONES = 2557\n",
      " 4000 : DONES = 2983\n",
      " 4500 : DONES = 3401\n",
      " 5000 : DONES = 3823\n",
      " 5500 : DONES = 4256\n",
      " 6000 : DONES = 4676\n",
      " 6500 : DONES = 5115\n",
      " 7000 : DONES = 5552\n",
      " 7500 : DONES = 5977\n",
      " 8000 : DONES = 6412\n",
      " 8500 : DONES = 6846\n",
      " 9000 : DONES = 7282\n",
      " 9500 : DONES = 7715\n",
      " 10000 : DONES = 8151\n",
      " 10500 : DONES = 8585\n",
      " 11000 : DONES = 9005\n",
      " 11500 : DONES = 9432\n",
      " 12000 : DONES = 9873\n",
      " 12500 : DONES = 10285\n",
      " 13000 : DONES = 10715\n",
      " 13500 : DONES = 11148\n",
      " 14000 : DONES = 11568\n",
      " 14500 : DONES = 11975\n",
      " 15000 : DONES = 12394\n",
      " 15500 : DONES = 12799\n",
      " 16000 : DONES = 13193\n",
      " 16500 : DONES = 13605\n",
      " 17000 : DONES = 14015\n",
      " 17500 : DONES = 14407\n",
      " 18000 : DONES = 14839\n",
      " 18500 : DONES = 15241\n",
      " 19000 : DONES = 15625\n",
      " 19500 : DONES = 16020\n",
      " 20000 : DONES = 16409\n",
      " 20500 : DONES = 16825\n",
      " 21000 : DONES = 17216\n",
      " 21500 : DONES = 17614\n",
      " 22000 : DONES = 18020\n",
      " 22500 : DONES = 18403\n",
      " 23000 : DONES = 18786\n",
      " 23500 : DONES = 19150\n",
      " 24000 : DONES = 19558\n",
      " 24500 : DONES = 19928\n",
      " 25000 : DONES = 20294\n",
      " 25500 : DONES = 20652\n",
      " 26000 : DONES = 21019\n",
      " 26500 : DONES = 21382\n",
      " 27000 : DONES = 21759\n",
      " 27500 : DONES = 22123\n",
      " 28000 : DONES = 22479\n",
      " 28500 : DONES = 22849\n",
      " 29000 : DONES = 23207\n",
      " 29500 : DONES = 23567\n",
      " 30000 : DONES = 23944\n",
      " 30500 : DONES = 24336\n",
      " 31000 : DONES = 24710\n",
      " 31500 : DONES = 25104\n",
      " 32000 : DONES = 25472\n",
      " 32500 : DONES = 25851\n",
      " 33000 : DONES = 26234\n",
      " 33500 : DONES = 26619\n",
      " 34000 : DONES = 26989\n",
      " 34500 : DONES = 27368\n",
      " 35000 : DONES = 27733\n",
      " 35500 : DONES = 28107\n",
      " 36000 : DONES = 28483\n",
      " 36500 : DONES = 28886\n",
      " 37000 : DONES = 29244\n",
      " 37500 : DONES = 29604\n",
      " 38000 : DONES = 29961\n",
      " 38500 : DONES = 30351\n",
      " 39000 : DONES = 30737\n",
      " 39500 : DONES = 31124\n",
      " 40000 : DONES = 31500\n",
      " 40500 : DONES = 31875\n",
      " 41000 : DONES = 32218\n",
      " 41500 : DONES = 32558\n",
      " 42000 : DONES = 32925\n",
      " 42500 : DONES = 33295\n",
      " 43000 : DONES = 33673\n",
      " 43500 : DONES = 34056\n",
      " 44000 : DONES = 34410\n",
      " 44500 : DONES = 34783\n",
      " 45000 : DONES = 35154\n",
      " 45500 : DONES = 35521\n",
      " 46000 : DONES = 35887\n",
      " 46500 : DONES = 36270\n",
      " 47000 : DONES = 36644\n",
      " 47500 : DONES = 37026\n",
      " 48000 : DONES = 37399\n",
      " 48500 : DONES = 37768\n",
      " 49000 : DONES = 38136\n",
      " 49500 : DONES = 38498\n",
      " 50000 : DONES = 38852\n",
      " 50500 : DONES = 39214\n",
      " 51000 : DONES = 39604\n",
      " 51500 : DONES = 39972\n",
      " 52000 : DONES = 40342\n",
      " 52500 : DONES = 40684\n",
      " 53000 : DONES = 41019\n",
      " 53500 : DONES = 41366\n",
      " 54000 : DONES = 41717\n",
      " 54500 : DONES = 42090\n",
      " 55000 : DONES = 42444\n",
      " 55500 : DONES = 42820\n",
      " 56000 : DONES = 43180\n",
      " 56500 : DONES = 43529\n",
      " 57000 : DONES = 43898\n",
      " 57500 : DONES = 44265\n",
      " 58000 : DONES = 44613\n",
      " 58500 : DONES = 44958\n",
      " 59000 : DONES = 45303\n",
      " 59500 : DONES = 45659\n",
      " 60000 : DONES = 46023\n",
      " 60500 : DONES = 46389\n",
      " 61000 : DONES = 46759\n",
      " 61500 : DONES = 47125\n",
      " 62000 : DONES = 47493\n",
      " 62500 : DONES = 47880\n",
      " 63000 : DONES = 48247\n",
      " 63500 : DONES = 48602\n",
      " 64000 : DONES = 48991\n",
      " 64500 : DONES = 49362\n",
      " 65000 : DONES = 49722\n",
      " 65500 : DONES = 50087\n",
      " 66000 : DONES = 50442\n",
      " 66500 : DONES = 50804\n",
      " 67000 : DONES = 51169\n",
      " 67500 : DONES = 51520\n",
      " 68000 : DONES = 51895\n",
      " 68500 : DONES = 52240\n",
      " 69000 : DONES = 52595\n",
      " 69500 : DONES = 52965\n",
      " 70000 : DONES = 53332\n",
      " 70500 : DONES = 53699\n",
      " 71000 : DONES = 54082\n",
      " 71500 : DONES = 54441\n",
      " 72000 : DONES = 54814\n",
      " 72500 : DONES = 55170\n",
      " 73000 : DONES = 55531\n",
      " 73500 : DONES = 55885\n",
      " 74000 : DONES = 56223\n",
      " 74500 : DONES = 56558\n",
      " 75000 : DONES = 56918\n",
      " 75500 : DONES = 57281\n",
      " 76000 : DONES = 57626\n",
      " 76500 : DONES = 57956\n",
      " 77000 : DONES = 58289\n",
      " 77500 : DONES = 58625\n",
      " 78000 : DONES = 58949\n",
      " 78500 : DONES = 59290\n",
      " 79000 : DONES = 59648\n",
      " 79500 : DONES = 59982\n",
      " 80000 : DONES = 60348\n",
      " 80500 : DONES = 60682\n",
      " 81000 : DONES = 61022\n",
      " 81500 : DONES = 61347\n",
      " 82000 : DONES = 61682\n",
      " 82500 : DONES = 62043\n",
      " 83000 : DONES = 62380\n",
      " 83500 : DONES = 62739\n",
      " 84000 : DONES = 63067\n",
      " 84500 : DONES = 63378\n",
      " 85000 : DONES = 63718\n",
      " 85500 : DONES = 64054\n",
      " 86000 : DONES = 64410\n",
      " 86500 : DONES = 64732\n",
      " 87000 : DONES = 65087\n",
      " 87500 : DONES = 65393\n",
      " 88000 : DONES = 65719\n",
      " 88500 : DONES = 66036\n",
      " 89000 : DONES = 66375\n",
      " 89500 : DONES = 66718\n",
      " 90000 : DONES = 67056\n",
      " 90500 : DONES = 67410\n",
      " 91000 : DONES = 67752\n",
      " 91500 : DONES = 68104\n",
      " 92000 : DONES = 68460\n",
      " 92500 : DONES = 68830\n",
      " 93000 : DONES = 69177\n",
      " 93500 : DONES = 69553\n",
      " 94000 : DONES = 69923\n",
      " 94500 : DONES = 70287\n",
      " 95000 : DONES = 70642\n",
      " 95500 : DONES = 70977\n",
      " 96000 : DONES = 71312\n",
      " 96500 : DONES = 71659\n",
      " 97000 : DONES = 72027\n",
      " 97500 : DONES = 72365\n",
      " 98000 : DONES = 72746\n",
      " 98500 : DONES = 73097\n",
      " 99000 : DONES = 73440\n",
      " 99500 : DONES = 73782\n"
     ]
    }
   ],
   "source": [
    "training(model, target_model,EPSILON,EPSILON_REDUCE,EPOCHS = 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46f0e64",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ace7161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_folder_path = './model'\n",
    "file_name = \"model_4.pth\"\n",
    "if not os.path.exists(model_folder_path):\n",
    "    os.makedirs(model_folder_path)        \n",
    "file_name = os.path.join(model_folder_path, file_name)\n",
    "torch.save(model.state_dict(), file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcddf02",
   "metadata": {},
   "source": [
    "## Let's play a game and see how it behaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "55e5e82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* * * * * * * * * * * * * * * \n",
      "*     M                     * \n",
      "*                           * \n",
      "*                           * \n",
      "*                           * \n",
      "*                           * \n",
      "*                           * \n",
      "*                           * \n",
      "*             *             * \n",
      "*                           * \n",
      "*     *                     * \n",
      "*                           * \n",
      "*                           * \n",
      "*                           * \n",
      "* * * * * * * * * * * * * * * \n"
     ]
    }
   ],
   "source": [
    "MyEnv = env(15) # initialization of environment\n",
    "state, done, reward = MyEnv.getFeature()\n",
    "num_done = 0  \n",
    "while not done:\n",
    "    # Clear the previous output\n",
    "    clear_output(wait=True)\n",
    "    # choose an action\n",
    "    action = epsilon_greedy_action_selection(model, epsilon = 0, observation = state, sharpening_factor=3)\n",
    "    # perform action and get next state\n",
    "    MyEnv.step(action)\n",
    "    new_state, done, reward = MyEnv.getFeature()\n",
    "\n",
    "    state = new_state\n",
    "    MyEnv.showField()\n",
    "\n",
    "    # Pause for a short duration \n",
    "    time.sleep(1)\n",
    "\n",
    "    # Display the updated content\n",
    "    display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d30556",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
